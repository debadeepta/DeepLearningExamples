{
    // Use IntelliSense to learn about possible attributes.
    // Hover to view descriptions of existing attributes.
    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
    "version": "0.2.0",
    "configurations": [
        
        {
            "name": "Python: Current File",
            "type": "python",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal"
        },
        {
            "name": "Train TransformerXL Distributed WT103",
            "type": "python",
            "request": "launch",
            "module": "torch.distributed.launch",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}/PyTorch/LanguageModeling/Transformer-XL/pytorch",
            "args": [
                "--nproc_per_node=1",
                "train.py",
                "--config",
                "dgx1_1gpu_fp32",
                "--config_file",
                "wt103_base.yaml"
            ],
            "env": {
                "NCCL_P2P_DISABLE": "1"
            }
        },
        {
            "name": "Train TransformerXL Distributed LM1B Large 1 GPU",
            "type": "python",
            "request": "launch",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}/PyTorch/LanguageModeling/Transformer-XL/pytorch",
            "program": "train.py",
            "args": [
                "--cuda",
                "--data",  "../data/one-billion-words/",
                "--dataset", "lm1b",
                "--adaptive", 
                "--div_val", "4",
                "--n_layer",  "24",
                "--d_model", "1280",
                "--n_head", "16",
                "--d_head", "80",
                "--d_inner", "8192",
                "--dropout", "0.05",
                "--dropatt", "0.05",
                "--optim", "adam",
                "--warmup_step", "30000",
                "--max_step", "1200000",
                "--lr", "0.00025",
                "--tgt_len", "32",
                "--mem_len", "32",
                "--eval_tgt_len", "32",
                "--batch_size", "512",
                "--multi_gpu", "ddp",
                "--gpu0_bsz", "0",
            ],
            "env": {
                "NCCL_P2P_DISABLE": "1"
            }
        },
        {
            "name": "Train TransformerXL Distributed LM1B Base 1 GPU",
            "type": "python",
            "request": "launch",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}/PyTorch/LanguageModeling/Transformer-XL/pytorch",
            "program": "train.py",
            "args": [
                "--cuda",
                "--data",  "../data/one-billion-words/",
                "--dataset", "lm1b",
                "--adaptive", 
                "--div_val", "4",
                "--n_layer",  "18",
                "--d_model", "1280",
                "--n_head", "8",
                "--d_head", "128",
                "--d_inner", "4096",
                "--dropout", "0.00",
                "--dropatt", "0.00",
                "--optim", "adam",
                "--warmup_step", "20000",
                "--max_step", "500000",
                "--lr", "0.00025",
                "--tgt_len", "32",
                "--mem_len", "32",
                "--eval_tgt_len", "32",
                "--batch_size", "224",
                "--multi_gpu", "ddp",
                "--gpu0_bsz", "32",
            ],
            "env": {
                "NCCL_P2P_DISABLE": "1"
            }
        },
        {
            "name": "Train TransformerXL Distributed LM1B Base 4 GPU",
            "type": "python",
            "request": "launch",
            "module": "torch.distributed.launch",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}/PyTorch/LanguageModeling/Transformer-XL/pytorch",
            "args": [
                "--nproc_per_node=4",
                "train.py",
                "--cuda",
                "--data",  "../data/one-billion-words/",
                "--dataset", "lm1b",
                "--adaptive", 
                "--div_val", "4",
                "--n_layer",  "18",
                "--d_model", "1280",
                "--n_head", "8",
                "--d_head", "128",
                "--d_inner", "4096",
                "--dropout", "0.00",
                "--dropatt", "0.00",
                "--optim", "adam",
                "--warmup_step", "20000",
                "--max_step", "500000",
                "--lr", "0.00025",
                "--tgt_len", "32",
                "--mem_len", "32",
                "--eval_tgt_len", "32",
                "--batch_size", "224",
                "--multi_gpu", "ddp",
                "--gpu0_bsz", "32",
            ],
            "env": {
                "NCCL_P2P_DISABLE": "1"
            }
        }
        

        
    ]
}